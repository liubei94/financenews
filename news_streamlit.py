import streamlit as st
import os
from datetime import datetime
from io import BytesIO

from news import (
    extract_article,
    extract_keywords_with_gpt,
    search_news_naver,
    extract_article,
    summarize_news_articles,
    save_summary_to_word,
    filter_news_by_date  # âœ… ë‚ ì§œ í•„í„°ë§ í•¨ìˆ˜ ì¶”ê°€ import
)

st.set_page_config(page_title="ë‰´ìŠ¤ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±ê¸°", page_icon="ğŸ“°")
st.title("ğŸ“° ë‰´ìŠ¤ ìš”ì•½ Word ë¦¬í¬íŠ¸ ìƒì„±ê¸°")
st.markdown("""
1. ë‰´ìŠ¤ ë§í¬ë¥¼ ì…ë ¥í•˜ë©´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³   
2. ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•œ ë’¤  
3. GPTë¡œ ìš”ì•½í•˜ì—¬  
4. Word íŒŒì¼ë¡œ ì •ë¦¬í•´ë“œë¦½ë‹ˆë‹¤.
""")

link = st.text_input("ğŸ”— ë¶„ì„í•  ë‰´ìŠ¤ ë§í¬ ì…ë ¥")
start_date = st.date_input("ê²€ìƒ‰ ì‹œì‘ì¼", datetime.today())
end_date = st.date_input("ê²€ìƒ‰ ì¢…ë£Œì¼", datetime.today())
count = st.number_input("ê²€ìƒ‰í•  ë‰´ìŠ¤ ê±´ìˆ˜", min_value=1, max_value=100, value=30)
save_filename = st.text_input("ğŸ’¾ ì €ì¥í•  íŒŒì¼ ì´ë¦„ (í™•ì¥ì ì œì™¸)", "ìš”ì•½_ë‰´ìŠ¤")

# ì´ˆê¸°í™”
if 'step' not in st.session_state:
    st.session_state.step = None

# ë‹¨ê³„ 1: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘
if st.button("ğŸš€ GPT í‚¤ì›Œë“œ ì¶”ì¶œ"):
    if not link:
        st.warning("ë‰´ìŠ¤ ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif start_date > end_date:
        st.error("ì¢…ë£Œì¼ì€ ì‹œì‘ì¼ë³´ë‹¤ ê°™ê±°ë‚˜ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("GPTë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                title, content = extract_article(link)
                default_keywords = extract_keywords_with_gpt(title, content)
                st.session_state.title = title
                st.session_state.content = content
                st.session_state.default_keywords = default_keywords
                st.session_state.step = "keywords_ready"
            except Exception as e:
                st.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                st.session_state.step = None

# ë‹¨ê³„ 2: í‚¤ì›Œë“œ í™•ì¸ ë° ë‰´ìŠ¤ ê²€ìƒ‰
if st.session_state.step == "keywords_ready":
    st.markdown("### ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ í™•ì¸ ë° ìˆ˜ì •")
    keywords_input = st.text_input(
        "GPTê°€ ì¶”ì¶œí•œ í‚¤ì›Œë“œì…ë‹ˆë‹¤. í•„ìš”ì‹œ ìˆ˜ì •í•˜ì„¸ìš” (ìµœëŒ€ 10ê°œ, ë„ì–´ì“°ê¸°ë¡œ êµ¬ë¶„):",
        value=', '.join(st.session_state.default_keywords),
        key="keywords_input"
    )

    if st.button("ğŸ“¡ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ìš”ì•½ ìƒì„±"):
        with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìš”ì•½ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                keywords = [kw.strip() for kw in keywords_input.split() if kw.strip()]
                if len(keywords) > 10:
                    st.warning("âš ï¸ í‚¤ì›Œë“œëŠ” ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
                    keywords = keywords[:10]

                # âœ… ë‚ ì§œ í¬ë§· ë³€ê²½ (filter í•¨ìˆ˜ì™€ ì¼ì¹˜í•˜ë„ë¡)
                s_date = start_date.strftime("%Y-%m-%d")
                e_date = end_date.strftime("%Y-%m-%d")

                # ë‰´ìŠ¤ ê²€ìƒ‰
                news_items = search_news_naver(keywords, s_date, e_date, count)

                # âœ… ë‚ ì§œ í•„í„°ë§ ì ìš©
                filtered_items = filter_news_by_date(news_items, s_date, e_date)

                if not filtered_items:
                    st.warning("âŒ ë‚ ì§œ ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # âœ… ë§í¬ ì¶”ì¶œë„ í•„í„°ë§ëœ ë‰´ìŠ¤ ê¸°ì¤€
                    links = [item['link'] for item in filtered_items]

                    titles, contents, failed_links = [], [], []
                    progress = st.progress(0)
                    status = st.empty()
                    for i, news_link in enumerate(links, 1):
                        status.text(f"í¬ë¡¤ë§ ì¤‘: [{i}/{len(links)}] {news_link}")
                        title, content = extract_article(news_link)
                        if title and content:
                            titles.append(title)
                            contents.append(content)
                        else:
                            failed_links.append(news_link)
                        progress.progress(i / len(links))

                    summary = summarize_news_articles(titles, contents)
                    st.subheader("ğŸ“ ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°")
                    st.markdown(f"<div style='white-space: pre-wrap'>{summary}</div>", unsafe_allow_html=True)

                    # Word íŒŒì¼ ìƒì„± (BytesIOë¡œ ì €ì¥)
                    buffer = BytesIO()
                    save_summary_to_word(
                        summary,
                        titles,
                        links,
                        filtered_items,  # âœ… í•„í„°ë§ëœ ë‰´ìŠ¤ë§Œ ì €ì¥
                        keywords,
                        output_stream=buffer,
                        failed_links=failed_links
                    )
                    buffer.seek(0)

                    st.success("âœ… Word íŒŒì¼ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.info("ğŸ’¡ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ íŒŒì¼ì´ ë¸Œë¼ìš°ì €ì˜ **ë‹¤ìš´ë¡œë“œ í´ë”**ì— ì €ì¥ë©ë‹ˆë‹¤.\n\nğŸ“ ì €ì¥ ìœ„ì¹˜ë¥¼ ì§ì ‘ ì§€ì •í•˜ê³  ì‹¶ë‹¤ë©´, ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ 'í•­ìƒ ì €ì¥ ìœ„ì¹˜ ë¬»ê¸°' ì˜µì…˜ì„ ì¼œì£¼ì„¸ìš”.")

                    st.download_button(
                        label="ğŸ“¥ ìš”ì•½ Word íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=buffer,
                        file_name=save_filename + ".docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )

                    if failed_links:
                        with st.expander("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ë‰´ìŠ¤ ë§í¬ ëª©ë¡"):
                            for fl in failed_links:
                                st.markdown(f"- {fl}")
            except Exception as e:
                st.error(f"ğŸš« ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
