import streamlit as st
import os
from datetime import datetime
from tempfile import NamedTemporaryFile

from news import (
    extract_article_content,
    extract_keywords_with_gpt,
    search_news_naver,
    extract_article,
    summarize_news_articles,
    save_summary_to_word
)

st.set_page_config(page_title="ë‰´ìŠ¤ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±ê¸°", page_icon="ğŸ“°")
st.title("ğŸ“° ë‰´ìŠ¤ ìš”ì•½ Word ë¦¬í¬íŠ¸ ìƒì„±ê¸°")
st.markdown("""
1. ë‰´ìŠ¤ ë§í¬ë¥¼ ì…ë ¥í•˜ë©´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³   
2. ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•œ ë’¤  
3. GPTë¡œ ìš”ì•½í•˜ì—¬  
4. Word íŒŒì¼ë¡œ ì •ë¦¬í•´ë“œë¦½ë‹ˆë‹¤.
""")

link = st.text_input("ğŸ”— ë¶„ì„í•  ë‰´ìŠ¤ ë§í¬ ì…ë ¥")
start_date = st.date_input("ê²€ìƒ‰ ì‹œì‘ì¼", datetime.today())
end_date = st.date_input("ê²€ìƒ‰ ì¢…ë£Œì¼", datetime.today())
count = st.number_input("ê²€ìƒ‰í•  ë‰´ìŠ¤ ê±´ìˆ˜", min_value=1, max_value=100, value=30)
save_filename = st.text_input("ğŸ’¾ ì €ì¥í•  íŒŒì¼ ì´ë¦„ (í™•ì¥ì ì œì™¸)", "ìš”ì•½_ë‰´ìŠ¤")
save_dir = st.text_input("ğŸ“ ì €ì¥ ê²½ë¡œ (ì—†ìœ¼ë©´ í˜„ì¬ í´ë”)", "")

if st.button("ğŸš€ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±í•˜ê¸°"):
    if not link:
        st.warning("ë‰´ìŠ¤ ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif start_date > end_date:
        st.error("ì¢…ë£Œì¼ì€ ì‹œì‘ì¼ë³´ë‹¤ ê°™ê±°ë‚˜ ì´í›„ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("GPT ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                title, content = extract_article_content(link)
                keywords = extract_keywords_with_gpt(title, content)
                st.markdown(f"ğŸ”‘ **ì¶”ì¶œëœ í‚¤ì›Œë“œ:** {' | '.join(keywords)}")

                s_date = start_date.strftime("%Y%m%d")
                e_date = end_date.strftime("%Y%m%d")
                news_items = search_news_naver(keywords, s_date, e_date, count)
                links = [item['link'] for item in news_items]

                titles, contents = [], []
                progress = st.progress(0)
                status = st.empty()
                for i, link in enumerate(links, 1):
                    status.text(f"í¬ë¡¤ë§ ì¤‘: [{i}/{len(links)}] {link}")
                    title, content = extract_article(link)
                    if title and content:
                        titles.append(title)
                        contents.append(content)
                    else:
                        st.warning(f"âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {link}")
                    progress.progress(i / len(links))

                summary = summarize_news_articles(titles, contents)
                st.subheader("ğŸ“ ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°")
                st.markdown(f"<div style='white-space: pre-wrap'>{summary}</div>", unsafe_allow_html=True)

                filename = save_filename + ".docx"
                save_path = os.path.join(save_dir if save_dir else os.getcwd(), filename)
                save_summary_to_word(summary, titles, links, news_items, keywords, save_path)

                with open(save_path, "rb") as f:
                    st.success("ìš”ì•½ Word ë¦¬í¬íŠ¸ê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.download_button(
                        label="ğŸ“¥ ìš”ì•½ Word íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=f,
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )

            except Exception as e:
                st.error(f"ğŸš« ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
