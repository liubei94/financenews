import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import os
from dotenv import load_dotenv
from docx import Document
from docx.shared import Pt
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.opc.constants import RELATIONSHIP_TYPE as RT
from urllib.parse import urlparse
from datetime import datetime
import re

# Load environment variables
load_dotenv()
client = OpenAI()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

### 1ë‹¨ê³„: ë‰´ìŠ¤ ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
def extract_article_content(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    title_tag = soup.find('h2', class_='media_end_head_headline')
    title = title_tag.get_text(strip=True) if title_tag else 'ì œëª© ì—†ìŒ'
    content_tag = soup.find('article')
    paragraphs = content_tag.find_all('p') if content_tag else []
    content = ' '.join([p.get_text(strip=True) for p in paragraphs])
    return title, content

### í‚¤ì›Œë“œ ì¶”ì¶œ (GPT)
def extract_keywords_with_gpt(title, content):
    prompt = f"""
ì œëª©ê³¼ ë³¸ë¬¸ì„ ì°¸ê³ í•´ í•µì‹¬ í‚¤ì›Œë“œ 5ê°œë¥¼ í•œê¸€ë¡œ ì¶”ì¶œí•´ì¤˜:
ì œëª©: {title}
ë³¸ë¬¸: {content}
"""
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ í‚¤ì›Œë“œ ì¶”ì¶œê¸°ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
    )
    keywords = response.choices[0].message.content.strip().split('\n')
    cleaned = []
    for kw in keywords:
        kw = re.sub(r'^\d+\.\s*', '', kw).strip()  # ìˆ«ì. ì œê±° (1. í‚¤ì›Œë“œ â†’ í‚¤ì›Œë“œ)
        if kw:
            cleaned.append(kw)
    return cleaned[:10]  # ìµœëŒ€ 10ê°œ ì œí•œ

### 2ë‹¨ê³„: ë‰´ìŠ¤ ê²€ìƒ‰ (NAVER API)
def search_news_naver(keywords, start_date, end_date, display=30):
    query = ' '.join([kw.strip("1234567890. ") for kw in keywords if kw.strip()])
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {
        "query": query,
        "display": display,
        "sort": "date"
    }
    response = requests.get(url, headers=headers, params=params)
    if response.status_code == 200:
        return response.json()['items']
    else:
        print("âš ï¸ ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨:", response.text)
        return []

### 3ë‹¨ê³„: ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§
def extract_naver_article(link):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
            'Referer': 'https://www.naver.com/',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }
        res = requests.get(link, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')

        title_tag = soup.find('meta', property='og:title')
        title = title_tag['content'].strip() if title_tag else 'ì œëª© ì—†ìŒ'

        content_area = soup.find('div', id='dic_area')
        if content_area:
            paragraphs = content_area.find_all(['p', 'span'])
            content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            return title, content

        origin_link_tag = soup.find('a', class_='media_end_head_origin_link')
        if origin_link_tag and origin_link_tag.get('href'):
            return extract_generic_article(origin_link_tag['href'])

        raise ValueError("ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ê³ , ê¸°ì‚¬ ì›ë¬¸ ë§í¬ë„ ì—†ìŒ.")
    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {link}\n{e}")
        return None, None

def extract_generic_article(link):
    try:
        res = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        soup = BeautifulSoup(res.text, 'html.parser')

        title_tag = soup.find("meta", property="og:title")
        title = title_tag["content"].strip() if title_tag and title_tag.get("content") else soup.title.string.strip()

        domain = urlparse(link).netloc
        if 'chosun.com' in domain:
            content_area = soup.find('div', id='news_body_area')
        elif 'donga.com' in domain:
            content_area = soup.find('div', class_='article_txt')
        elif 'mk.co.kr' in domain:
            content_area = soup.find('div', class_='art_txt')
        elif 'joongang.co.kr' in domain:
            content_area = soup.find('div', id='article_body')
        elif 'hani.co.kr' in domain:
            content_area = soup.find('div', class_='article-text')
        else:
            content_area = None

        paragraphs = content_area.find_all('p') if content_area else soup.find_all('p')
        content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 30])
        return title, content
    except Exception as e:
        print(f"âŒ ì™¸ë¶€ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {link}\n{e}")
        return None, None

def extract_article(link):
    return extract_naver_article(link) if 'n.news.naver.com' in link else extract_generic_article(link)

### 4ë‹¨ê³„: ë‰´ìŠ¤ ìš”ì•½ (GPT)
def summarize_news_articles(titles, contents):
    full_text = ""
    for i in range(len(titles)):
        full_text += f"[{i+1}] {titles[i]}\n{contents[i]}\n\n"

    prompt = f"""
ë‹¹ì‹ ì€ ê²½ì œ/ì‚°ì—… ë¶„ì•¼ì˜ ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì…ë‹ˆë‹¤.  
ì´ ë‚´ìš©ì„ **ì‹¬ì¸µ ë¶„ì„ ìš”ì•½** í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”. ìš”ì•½ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:

---

1. ğŸ“Œ **í•µì‹¬ ì£¼ì œ ìš”ì•½** (1~2ë¬¸ì¥)

2. ğŸ“° **ë‰´ìŠ¤ ìš”ì  ì •ë¦¬**
   - ì–´ë–¤ ì‚¬ê±´/í–‰ë™ì´ ìˆì—ˆëŠ”ê°€?
   - ì£¼ìš” ì¸ë¬¼, ê¸°ì—…, ê¸°ê´€ì€ ëˆ„êµ¬ì¸ê°€?
   - ê¸°ìˆ /ì‚°ì—…/ì‹œì¥ ë§¥ë½ì€ ë¬´ì—‡ì¸ê°€?

3. ğŸ“Š **ë¹„êµ ë˜ëŠ” ì´ìŠˆ ìš”ì•½ (í•„ìš”ì‹œ í‘œë¡œ)**  
   - ê¸°ì‚¬ ê°„ ìœ ì‚¬ì /ì°¨ì´ì  ì •ë¦¬
   - ìˆ˜ì¹˜/ì •ì±… ë³€í™” ë¹„êµ ë“±

4. ğŸ§  **ê²°ë¡  ë° ì‹œì‚¬ì **
   - í–¥í›„ ì£¼ì˜ ê¹Šê²Œ ë´ì•¼ í•  ë³€í™”ë‚˜ íë¦„
   - ë…ìê°€ ì–»ì„ ìˆ˜ ìˆëŠ” í†µì°°

---

ì•„ë˜ëŠ” ë¶„ì„í•  ë‰´ìŠ¤ ì „ì²´ ë‚´ìš©ì…ë‹ˆë‹¤:

{full_text}

âš ï¸ ëˆ„ë½ëœ ìŸì ì´ë‚˜ ë³´ì™„ ì„¤ëª…ì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ë§ˆì§€ë§‰ì— ë”°ë¡œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì •í™•í•˜ê³  ê¹Šì´ ìˆëŠ” ë‰´ìŠ¤ ìš”ì•½ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

### 5ë‹¨ê³„: Word íŒŒì¼ ì €ì¥
def save_summary_to_word(summary_text, titles, links, news_items, keywords, output_stream, failed_links=None):
    doc = Document()
    style = doc.styles['Normal']
    font = style.font
    font.name = 'ë§‘ì€ ê³ ë”•'
    font.size = Pt(10)

    doc.add_paragraph("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ: " + ', '.join(keywords))
    doc.add_paragraph("")

    lines = summary_text.split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue
        if line.startswith("ì œëª©:"):
            p = doc.add_paragraph()
            run = p.add_run(line)
            run.bold = True
            run.font.size = Pt(14)
        elif line.startswith("ê²°ë¡ :") or line.startswith("ê²°ë¡ "):
            doc.add_paragraph("")
            p = doc.add_paragraph()
            run = p.add_run(line)
            run.bold = True
            run.font.size = Pt(10)
        elif line.startswith('|') and line.endswith('|'):
            table_data = []
            while line.startswith('|') and line.endswith('|'):
                cells = [cell.strip() for cell in line.strip().split('|')[1:-1]]
                table_data.append(cells)
                lines = lines[1:]
                if not lines:
                    break
                line = lines[0].strip()
            table = doc.add_table(rows=0, cols=len(table_data[0]))
            for row_data in table_data:
                row = table.add_row().cells
                for idx, cell in enumerate(row_data):
                    row[idx].text = cell
            continue
        else:
            p = doc.add_paragraph(line)
            p.style.font.size = Pt(10)

    doc.add_page_break()
    doc.add_paragraph("ğŸ“ ì°¸ê³  ë‰´ìŠ¤ ëª©ë¡", style='Heading 1')

    for idx, (title, link, item) in enumerate(zip(titles, links, news_items), 1):
        p = doc.add_paragraph()
        p.add_run(f"{idx}. ")
        add_hyperlink(p, link, title)
        origin = extract_news_source(link)
        pubdate = extract_pubdate_from_item(item)
        info = f" ({origin}" + (f", {pubdate}" if pubdate else "") + ")"
        p.add_run(info)

    doc.save(output_stream)
    print(f"âœ… Word íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_stream}")

def add_hyperlink(paragraph, url, text):
    part = paragraph.part
    r_id = part.relate_to(url, RT.HYPERLINK, is_external=True)

    hyperlink = OxmlElement('w:hyperlink')
    hyperlink.set(qn('r:id'), r_id)

    new_run = OxmlElement('w:r')
    rPr = OxmlElement('w:rPr')

    color = OxmlElement('w:color')
    color.set(qn('w:val'), '0000FF')
    rPr.append(color)

    underline = OxmlElement('w:u')
    underline.set(qn('w:val'), 'single')
    rPr.append(underline)

    new_run.append(rPr)

    text_elem = OxmlElement('w:t')
    text_elem.text = text
    new_run.append(text_elem)
    hyperlink.append(new_run)
    paragraph._p.append(hyperlink)

def extract_news_source(link):
    netloc = urlparse(link).netloc
    domain = netloc.replace("www.", "").split(".")[0]
    source_map = {
        "n": "ë„¤ì´ë²„",
        "chosun": "ì¡°ì„ ì¼ë³´",
        "donga": "ë™ì•„ì¼ë³´",
        "mk": "ë§¤ì¼ê²½ì œ",
        "joongang": "ì¤‘ì•™ì¼ë³´",
        "hani": "í•œê²¨ë ˆ",
        "yna": "ì—°í•©ë‰´ìŠ¤",
        "inews24": "ì•„ì´ë‰´ìŠ¤24"
    }
    return source_map.get(domain, domain)

def extract_pubdate_from_item(item):
    if "pubDate" in item:
        try:
            dt = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d")
        except:
            return None
    return None
