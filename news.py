# news.py (Refactored and Unified Version)

import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import os
from dotenv import load_dotenv
from docx import Document
from docx.shared import Pt
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.opc.constants import RELATIONSHIP_TYPE as RT
from urllib.parse import urlparse
from datetime import datetime
import re
import time
import sys

# --- Selenium
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# Load environment variables
load_dotenv()
client = OpenAI()

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

# ğŸ”¹ HTML â†’ Markdown ë³€í™˜ê¸°
from html_to_markdown import convert_to_markdown

# âœ… ìƒˆë¡­ê²Œ ì •ì˜ëœ ê¸°ì‚¬ ì¶”ì¶œ í•¨ìˆ˜
def extract_article(url):
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    except Exception as e:
        print(f"[ì˜¤ë¥˜] Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
        return None, None

    try:
        print(f"\n[extract_article] '{url}' ë¡œë”© ì¤‘...")
        driver.get(url)
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(3)
        html_content = driver.page_source
    except Exception as e:
        print(f"[ì˜¤ë¥˜] Selenium ë¡œë”© ì‹¤íŒ¨: {e}")
        driver.quit()
        return None, None
    finally:
        driver.quit()

    soup = BeautifulSoup(html_content, 'html.parser')
    article_body = None

    selectors = [
        'article', '#article_body', '#dic_area', '#article-view-content-div',
        '#main-content', '#content', '.article_body', '.entry-content', 'main',
    ]
    for selector in selectors:
        found = soup.select_one(selector)
        if found:
            article_body = found
            break
    if not article_body:
        article_body = soup.find('body')
    if not article_body:
        print("[extract_article] ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
        return None, None

    title_tag = article_body.find(['h1', 'h2', 'h3']) or soup.find(['h1', 'h2', 'h3']) or soup.find('title')
    page_title = title_tag.get_text(strip=True) if title_tag else "Untitled"
    markdown_content = convert_to_markdown(str(article_body))
    
    return page_title, markdown_content

# âœ… GPTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords_with_gpt(title, content):
    prompt = f"""
ë‹¤ìŒì€ ë‰´ìŠ¤ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì…ë‹ˆë‹¤. í•µì‹¬ í‚¤ì›Œë“œ 3ê°œë¥¼ í•œê¸€ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ì œëª©ì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë‚˜ í‘œí˜„ì„ ìš°ì„  ê³ ë ¤í•´ í‚¤ì›Œë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.
- ë³¸ë¬¸ ì „ì²´ë¥¼ ì°¸ê³ í•˜ë˜, ì£¼ì œë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë¥¼ ë½‘ì•„ì£¼ì„¸ìš”.

ì œëª©: {title}
ë³¸ë¬¸: {content}
"""
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ í‚¤ì›Œë“œ ì¶”ì¶œê¸°ì…ë‹ˆë‹¤. ë³¸ë¬¸ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ]
    )
    keywords = response.choices[0].message.content.strip().split('\n')
    cleaned = [re.sub(r'^\d+\.\s*', '', kw).strip() for kw in keywords if kw.strip()]
    return cleaned[:3]

# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰
def search_news_naver(keywords, start_date, end_date, display=30):
    query = ' '.join(keywords)
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {"query": query, "display": display, "sort": "date"}
    response = requests.get(url, headers=headers, params=params)
    if response.status_code == 200:
        return response.json().get('items', [])
    else:
        print(f"âš ï¸ ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {response.text}")
        return []

# âœ… ë‚ ì§œ í•„í„°ë§
def filter_news_by_date(news_items, start_date, end_date):
    start = datetime.strptime(start_date, "%Y-%m-%d").date()
    end = datetime.strptime(end_date, "%Y-%m-%d").date()
    filtered = []
    for item in news_items:
        pub_raw = item.get("pubDate")
        if not pub_raw: continue
        try:
            pub_date = datetime.strptime(pub_raw, "%a, %d %b %Y %H:%M:%S %z").date()
            if start <= pub_date <= end:
                filtered.append(item)
        except ValueError:
            print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {pub_raw}")
    return filtered

# âœ… GPTë¡œ ë‰´ìŠ¤ ìš”ì•½
def summarize_news_articles(titles, contents):
    full_text = ""
    for i, (title, content) in enumerate(zip(titles, contents)):
        full_text += f"[{i+1}] {title}\n{content[:800]}\n\n"

    prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì…ë‹ˆë‹¤.
ì´ ë‚´ìš©ì„ **ì‹¬ì¸µ ë¶„ì„ ìš”ì•½** í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”. ìš”ì•½ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:

---
1. ğŸ“Œ **í•µì‹¬ ì£¼ì œ ìš”ì•½** (1~2ë¬¸ì¥)
2. ğŸ“° **ë‰´ìŠ¤ ìš”ì  ì •ë¦¬** (ì–´ë–¤ ì‚¬ê±´/ì›ì¸/ì£¼ìš” ì¸ë¬¼ ë“±)
3. ğŸ“Š **ë¹„êµ ë˜ëŠ” ì´ìŠˆ ìš”ì•½ (í•„ìš”ì‹œ í‘œë¡œ)**
4. ğŸ§  **ê²°ë¡  ë° ì‹œì‚¬ì ** (í–¥í›„ ì „ë§, ë…ìì˜ í†µì°°)
---

ë¶„ì„í•  ë‰´ìŠ¤ ì „ì²´ ë‚´ìš©:
{full_text}
"""
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "ë‹¹ì‹ ì€ ì •í™•í•˜ê³  ê¹Šì´ ìˆëŠ” ë‰´ìŠ¤ ìš”ì•½ê°€ì…ë‹ˆë‹¤."}, {"role": "user", "content": prompt}],
        temperature=0.7
    )
    return response.choices[0].message.content.strip()

# âœ… Word ì €ì¥
def save_summary_to_word(summary_text, titles, links, news_items, keywords, output_stream, failed_links=None):
    doc = Document()
    style = doc.styles['Normal']
    font = style.font
    font.name = 'ë§‘ì€ ê³ ë”•'
    font.size = Pt(10)

    section_titles = ["í•µì‹¬ ì£¼ì œ ìš”ì•½", "ë‰´ìŠ¤ ìš”ì  ì •ë¦¬", "ë¹„êµ ë˜ëŠ” ì´ìŠˆ ìš”ì•½", "ê²°ë¡  ë° ì‹œì‚¬ì "]
    lines = summary_text.split('\n')
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if not line:
            i += 1; continue
        if line.startswith('|') and line.endswith('|'):
            table_data = []
            while i < len(lines) and lines[i].strip().startswith('|'):
                cells = [c.strip() for c in lines[i].strip().split('|')[1:-1]]
                table_data.append(cells)
                i += 1
            if not table_data: continue
            table = doc.add_table(rows=0, cols=len(table_data[0]))
            table.style = 'Table Grid'
            for row_data in table_data:
                row_cells = table.add_row().cells
                for idx, cell_text in enumerate(row_data):
                    row_cells[idx].text = cell_text
            continue
        matched = False
        for title in section_titles:
            if title in line:
                p = doc.add_paragraph()
                run = p.add_run(re.sub(r'^\d+\.\s*ğŸ“Œ?\s*', '', line).strip())
                run.bold = True
                run.font.size = Pt(14)
                matched = True
                break
        if matched:
            i += 1; continue
        p = doc.add_paragraph()
        parts = re.split(r'(\*\*.*?\*\*)', line)
        for part in parts:
            if part.startswith('**') and part.endswith('**'):
                run = p.add_run(part[2:-2])
                run.bold = True
                run.font.size = Pt(10)
            else:
                p.add_run(part)
        i += 1

    doc.add_page_break()
    doc.add_paragraph("ğŸ“ ì°¸ê³  ë‰´ìŠ¤ ëª©ë¡", style='Heading 1')
    for idx, (title, link, item) in enumerate(zip(titles, links, news_items), 1):
        p = doc.add_paragraph()
        p.add_run(f"{idx}. ")
        add_hyperlink(p, link, title)
        origin = extract_news_source(link)
        pubdate = extract_pubdate_from_item(item)
        info = f" ({origin}" + (f", {pubdate}" if pubdate else "") + ")"
        p.add_run(info)

    if failed_links:
        doc.add_paragraph("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨ ë§í¬", style='Heading 2')
        for link in failed_links:
            doc.add_paragraph(link, style='List Bullet')

    doc.save(output_stream)

# í•˜ì´í¼ë§í¬ ì¶”ê°€ ë„ìš°ë¯¸
def add_hyperlink(paragraph, url, text):
    part = paragraph.part
    r_id = part.relate_to(url, RT.HYPERLINK, is_external=True)
    hyperlink = OxmlElement('w:hyperlink')
    hyperlink.set(qn('r:id'), r_id)
    new_run = OxmlElement('w:r')
    rPr = OxmlElement('w:rPr')
    r_style = OxmlElement('w:rStyle')
    r_style.set(qn('w:val'), 'Hyperlink')
    rPr.append(r_style)
    new_run.append(rPr)
    new_run.text = text
    hyperlink.append(new_run)
    paragraph._p.append(hyperlink)

def extract_news_source(link):
    netloc = urlparse(link).netloc
    return netloc.replace("www.", "").replace("n.news.", "")

def extract_pubdate_from_item(item):
    if "pubDate" in item:
        try:
            dt = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d")
        except ValueError:
            return None
    return None

# âœ… ë©”ì¸ ì‹¤í–‰ íë¦„
if __name__ == "__main__":
    test_url = "https://n.news.naver.com/mnews/article/014/0005371160"
    start_date = "2025-07-28"
    end_date = "2025-07-30"

    print("â–¶ï¸ [1/5] ê¸°ì‚¬ ì›ë¬¸ ìˆ˜ì§‘ ì¤‘...")
    title, content = extract_article(test_url)
    print(f"   - ì›ë¬¸ ì œëª©: {title}")

    print("â–¶ï¸ [2/5] GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    keywords = extract_keywords_with_gpt(title, content)
    print(f"   - ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords)}")

    print("â–¶ï¸ [3/5] ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° í•„í„°ë§ ì¤‘...")
    news_items = search_news_naver(keywords, start_date, end_date)
    filtered_items = filter_news_by_date(news_items, start_date, end_date)
    print(f"   - ê²€ìƒ‰ëœ ê¸°ì‚¬ ìˆ˜: {len(filtered_items)}ê±´")

    if filtered_items:
        print("â–¶ï¸ [4/5] ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ìš”ì•½ ì¤‘...")
        links = [item['link'] for item in filtered_items]
        titles, contents, failed = [], [], []
        for link in links:
            t, c = extract_article(link)
            if t and c:
                titles.append(t)
                contents.append(c)
            else:
                failed.append(link)

        summary = summarize_news_articles(titles, contents)
        print("   - ìš”ì•½ ì™„ë£Œ.")

        print("â–¶ï¸ [5/5] Word íŒŒì¼ ì €ì¥ ì¤‘...")
        output_filename = "news_summary_output.docx"
        save_summary_to_word(summary, titles, links, filtered_items, keywords, output_filename, failed_links=failed)
        print(f"âœ… ìµœì¢… ë¦¬í¬íŠ¸ê°€ '{output_filename}'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âŒ ë‚ ì§œì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ì–´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
