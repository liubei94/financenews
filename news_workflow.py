import requests
from bs4 import BeautifulSoup
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv
from docx import Document
from docx.shared import Pt
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.opc.constants import RELATIONSHIP_TYPE as RT
from urllib.parse import urlparse
from datetime import datetime
import re
import asyncio
import httpx
from tqdm.asyncio import tqdm
import json

# --- [NEW] crawl4ai and Pydantic imports ---
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler
from crawl4ai.config import CrawlerRunConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai.config import LLMConfig
# ---------------------------------------------

# Load environment variables
load_dotenv()

# ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Google Gemini ì„¤ì • - í•„ìˆ˜ ì‚¬ìš©
try:
    import google.generativeai as genai
    # Using GOOGLE_API_KEY to align with genai and crawl4ai usage
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key or not google_api_key.strip():
        raise ValueError("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=google_api_key)
    print("âœ… Gemini API ì„¤ì • ì™„ë£Œ")
except ImportError:
    raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-generativeai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
except Exception as e:
    raise Exception(f"Gemini API ì„¤ì • ì‹¤íŒ¨: {e}")

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")


# --- Pydantic model for crawl4ai data structure ---
# LLMì—ê²Œ ì–´ë–¤ í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë½‘ì•„ë‚¼ì§€ ì•Œë ¤ì£¼ëŠ” 'ì„¤ê³„ë„'ì…ë‹ˆë‹¤.
class NewsArticle(BaseModel):
    title: str = Field(..., description="The main headline or title of the news article.")
    content: str = Field(..., description="The full body text of the news article, excluding ads, comments, and navigation links.")
# ----------------------------------------------------



### ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ (Streamlitì—ì„œ í˜¸ì¶œ)


# --- [MODIFIED] Replaced BeautifulSoup with crawl4ai for robust extraction ---
def extract_initial_article_content(url):
    """
    ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ê¸°ì¤€ì´ ë˜ëŠ” ì²« ê¸°ì‚¬ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    crawl4aië¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ë–¤ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë“  ì•ˆì •ì ìœ¼ë¡œ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    async def _async_extract(url: str):
        """Asynchronous helper function to run crawl4ai."""
        # [ìˆ˜ì •ë¨] cache_mode ëŒ€ì‹  use_cache ì‚¬ìš©
        config = CrawlerRunConfig(
            extraction_strategy=LLMExtractionStrategy(
                llm_config=LLMConfig(
                    provider="gemini/gemini-2.5-flash",
                    api_token=os.getenv("GOOGLE_API_KEY")
                ),
                schema=NewsArticle.model_json_schema(),
                instruction="""Extract the title and the main content of the news article.
                Focus only on the article's body, ignoring comments, related articles, and advertisements.
                Return the result in JSON format based on the provided schema.""",
            ),
            use_cache=True  # CacheMode.ENABLED -> use_cache=True
        )
        try:
            async with AsyncWebCrawler(verbose=False) as crawler:
                result = await crawler.arun(url=url, config=config)
            
            if result.success and result.extracted_content:
                extracted_data = json.loads(result.extracted_content)
                return extracted_data.get("title"), extracted_data.get("content")
            else:
                print(f"âŒ crawl4ai ì´ˆê¸° ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {result.error_message}")
                return None, None
        except Exception as e:
            print(f"âŒ crawl4ai ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return None, None

    try:
        title, content = asyncio.run(_async_extract(url))
        if not title or not content:
            raise Exception("crawl4ai failed to extract the initial article.")
        return title, content
    except Exception as e:
        print(f"âŒ ì´ˆê¸° ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        raise
# --------------------------------------------------------------------------


async def extract_keywords_with_gemini(title, content, max_count=5):
    """Geminië¥¼ ì‚¬ìš©í•´ ë¹„ë™ê¸°ì ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # Geminiì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ê³¼ ë™ì¼)
    prompt = f"""
ë‹¤ìŒì€ ë‰´ìŠ¤ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë¥¼ ìµœëŒ€ {max_count}ê°œê¹Œì§€ í•œê¸€ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ê° í‚¤ì›Œë“œëŠ” ëª…ì‚¬ í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.
- êµ¬ë¶„ìëŠ” ì‰¼í‘œ(,) ë˜ëŠ” ì¤„ë°”ê¿ˆì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë³¸ë¬¸: {content}
"""
    
    # --- [ìˆ˜ì •] Gemini API í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ ---
    def generate_keywords_sync():
        """Gemini APIë¥¼ ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            # GeminiëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ í•„ìˆ˜ê°€ ì•„ë‹ˆë¯€ë¡œ, ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë§Œ ì „ë‹¬
            contents = [prompt]
            generation_config = {"temperature": 0.2}

            response = model.generate_content(
                contents=contents,
                generation_config=generation_config
            )

            if not response.parts:
                print("âš ï¸ Gemini APIê°€ í‚¤ì›Œë“œì— ëŒ€í•´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                return ""
            return response.text.strip()
        except Exception as e:
            print(f"âŒ Gemini í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    try:
        keywords_text = await asyncio.to_thread(generate_keywords_sync)
        if not keywords_text:
            raise Exception("Geminië¡œë¶€í„° í‚¤ì›Œë“œë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # --- ê¸°ì¡´ê³¼ ë™ì¼í•œ í›„ì²˜ë¦¬ ë¡œì§ ---
        # AIê°€ ìƒì„±í•œ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ íŒŒì‹±í•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤.
        lines = keywords_text.split("\n")
        keywords_list = []
        for line in lines:
            keywords_list.extend([kw.strip() for kw in line.split(",")])

        cleaned_keywords = [
            re.sub(r"^\s*[\d\.\-]+\s*", "", kw).strip()
            for kw in keywords_list
            if kw.strip()
        ]

        return cleaned_keywords[:max_count]
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def search_news_naver(keywords, display=50):
    """ë„¤ì´ë²„ APIë¥¼ í†µí•´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    query = " ".join(keywords)
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {"query": query, "display": display, "sort": "date"}
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        return response.json()["items"]
    except requests.RequestException as e:
        print(f"âš ï¸ ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        raise


def filter_news_by_date(news_items, start_date, end_date):
    """ê²€ìƒ‰ëœ ë‰´ìŠ¤ë¥¼ ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤."""
    filtered_items = []
    for item in news_items:
        pub_date_str = item.get("pubDate")
        if not pub_date_str:
            continue
        try:
            pub_date = datetime.strptime(
                pub_date_str, "%a, %d %b %Y %H:%M:%S %z"
            ).date()
            if start_date <= pub_date <= end_date:
                filtered_items.append(item)
        except ValueError:
            continue
    return filtered_items


# --- ë¹„ë™ê¸° ì²˜ë¦¬ í•µì‹¬ ë¡œì§ ---


# --- [MODIFIED] Replaced BeautifulSoup with crawl4ai for robust extraction ---
async def extract_article_content_async(link: str):
    """
    crawl4aië¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # [ìˆ˜ì •ë¨] cache_mode ëŒ€ì‹  use_cache ì‚¬ìš©
    config = CrawlerRunConfig(
        extraction_strategy=LLMExtractionStrategy(
            llm_config=LLMConfig(
                provider="gemini/gemini-2.5-flash",
                api_token=google_api_key
            ),
            schema=NewsArticle.model_json_schema(),
            instruction="""Extract the title and the main content of the news article.
            Focus only on the article's body, ignoring comments, related articles, and advertisements.
            Return the result in JSON format based on the provided schema.""",
        ),
        use_cache=False # CacheMode.DISABLED -> use_cache=False
    )
    try:
        async with AsyncWebCrawler(verbose=False) as crawler:
            result = await crawler.arun(url=link, config=config)
        
        if result.success and result.extracted_content:
            extracted_data = json.loads(result.extracted_content)
            return extracted_data.get("title"), extracted_data.get("content")
        else:
            return None, None
    except Exception:
        return None, None
# --------------------------------------------------------------------------



async def summarize_individual_article_async(title, content):
    prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ì•„ë˜ í•­ëª©ì— ë§ì¶”ì–´ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜. ê° í•­ëª©ì€ 2~3 ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
- **ì‚¬ê±´/ì£¼ì œ**: \n- **ì£¼ìš” ì¸ë¬¼/ê¸°ê´€**: \n- **í•µì‹¬ ì£¼ì¥/ë‚´ìš©**: \n- **ê²°ê³¼/ì˜í–¥**:
---
ì œëª©: {title}\në³¸ë¬¸: {content}
"""
    # --- [ìˆ˜ì •] Gemini API í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ ---
    def generate_summary_sync():
        """Gemini APIë¥¼ ë™ê¸°ì ìœ¼ë¡œ í˜¸ì¶œí•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        try:
            # 1. Gemini ëª¨ë¸ ê°ì²´ ìƒì„± (2.5-flash ëª¨ë¸ ì‚¬ìš©)
            model = genai.GenerativeModel('gemini-2.5-flash')

            # 2. ì‹œìŠ¤í…œ ì§€ì‹œì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            system_prompt = "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ì˜ í•µì‹¬ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤."
            contents = [system_prompt, prompt]

            # 3. ìƒì„± ì˜µì…˜ ì„¤ì •
            generation_config = {"temperature": 0.2}

            # 4. API í˜¸ì¶œ
            response = model.generate_content(
                contents=contents,
                generation_config=generation_config
            )

            # 5. ì‘ë‹µ í…ìŠ¤íŠ¸ ë°˜í™˜ (ì•ˆì „ì¥ì¹˜ í¬í•¨)
            if not response.parts:
                print("âš ï¸ Gemini APIê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                return None
            return response.text.strip()
        except Exception as e:
            print(f"âŒ Gemini ê°œë³„ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    try:
        # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        summary = await asyncio.to_thread(generate_summary_sync)
        return summary
    except Exception:
        return None


async def process_article_task(item, semaphore):
    async with semaphore:
        link = item.get("originallink", item.get("link"))
        # [MODIFIED] Removed unused 'session' argument
        title, content = await extract_article_content_async(link)
        if not title or not content:
            return {"status": "failed", "reason": "í¬ë¡¤ë§ ì‹¤íŒ¨", "link": link}
        summary = await summarize_individual_article_async(title, content)
        if not summary:
            return {"status": "failed", "reason": "ê°œë³„ ìš”ì•½ ì‹¤íŒ¨", "link": link}
        return {
            "status": "success",
            "title": re.sub("<.*?>", "", item["title"]),
            "link": link,
            "original_item": item,
            "summary": summary,
        }


async def synthesize_final_report(summaries):
    """ëª¨ë“  ë‰´ìŠ¤ ìš”ì•½ë³¸ì„ ë°›ì•„ í•˜ë‚˜ì˜ ì¢…í•© ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # AI ì…ë ¥ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ€ ê¸€ì ìˆ˜ ì œí•œ (í† í° ì•½ 2ë§Œê°œ ë¶„ëŸ‰)
    MAX_INPUT_CHARS = 25000 
    
    full_summary_text = ""
    processed_count = 0
    for summary_data in summaries:
        summary_entry = f"### ë‰´ìŠ¤ {processed_count + 1}: {summary_data['title']}\n{summary_data['summary']}\n\n---\n\n"
        if len(full_summary_text) + len(summary_entry) > MAX_INPUT_CHARS:
            break
        full_summary_text += summary_entry
        processed_count += 1

    if processed_count < len(summaries):
        remaining_count = len(summaries) - processed_count
        full_summary_text += f"\n... ì™¸ {remaining_count}ê°œì˜ ê´€ë ¨ ê¸°ì‚¬ê°€ ìˆìœ¼ë‚˜, ì•ˆì •ì ì¸ ë¶„ì„ì„ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"

    system_prompt = """
ë‹¹ì‹ ì€ ì •ì¹˜/ê²½ì œ/ì‚°ì—… ë¶„ì•¼ì˜ ìµœê³  ìˆ˜ì¤€ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í•µì‹¬ ìš”ì•½ë³¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, íšŒì‚¬ CFOë‚˜ CEOê°€ ì˜ì‚¬ê²°ì •ì„ ìœ„í•´ ì°¸ê³ í•  ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

# ì§€ì‹œì‚¬í•­
ì•„ë˜ì˜ êµ¬ì¡°ì™€ ì„¸ë¶€ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì „ì²´ì ì¸ ë¶„ëŸ‰ì€ 2000ì ë‚´ì™¸ë¡œ, Key Developmentsì™€ Comparative Analysisì˜ ë¹„ì¤‘ì„ ê°ê° 40% ìˆ˜ì¤€ì„ ìœ ì§€í•©ë‹ˆë‹¤.
ë‹¨, ìµœì¢… ê²°ê³¼ë¬¼ì„ ì¶œë ¥í•  ë•Œ ëª©ë¡ì€ ë²ˆí˜¸(1., 2.)ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì˜¤ì§ ë¶ˆë › í¬ì¸íŠ¸('*')ë¡œë§Œ í†µì¼í•´ì•¼ í•©ë‹ˆë‹¤.

---
## ë³´ê³ ì„œ êµ¬ì¡° ë° ì„¸ë¶€ ì§€ì¹¨
1.  **ğŸ“Œ Executive Summary (í•µì‹¬ ìš”ì•½)**
    * ì „ì²´ ìƒí™©ì„ 1~2 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
2.  **ğŸ“° Key Developments (ì£¼ìš” ë™í–¥ ë° ì‚¬ì‹¤ ë¶„ì„)**
    * ì–´ë–¤ ì‚¬ê±´/í–‰ë™ì´ ìˆì—ˆëŠ”ì§€ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
    * ê³µí†µì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ëŠ” ì›ì¸ê³¼ ë°°ê²½ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
    * í•µì‹¬ì ì¸ ì´í•´ê´€ê³„ì(ì¸ë¬¼, ê¸°ì—…, ê¸°ê´€)ëŠ” ëˆ„êµ¬ì´ë©°, ê·¸ë“¤ì˜ ì…ì¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
3.  **ğŸ“Š Comparative Analysis (ë¹„êµ ë¶„ì„ ë° ì´ìŠˆ ì‹¬ì¸µ íƒêµ¬)**
    * ê¸°ì‚¬ë“¤ ê°„ì˜ ê´€ì  ì°¨ì´ë‚˜ ìƒì¶©ë˜ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
    * ìˆ˜ì¹˜, ë°ì´í„°, ì •ì±… ë³€í™” ë“± ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ í‘œ(Table) í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ì‹œê°ì  ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤. (í•„ìš”ì‹œ)
4.  **ğŸ§  Conclusion & Strategic Implications (ê²°ë¡  ë° ì „ëµì  ì‹œì‚¬ì )**
    * Key Developmentsì™€ Comparative Analysisì˜ ë‚´ìš©ì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•©ë‹ˆë‹¤.
    * ì´ëŸ¬í•œ íë¦„ì´ í–¥í›„ ì‹œì¥/ì‚°ì—…/ì •ì±…ì— ë¯¸ì¹  ì˜í–¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
    * ìš°ë¦¬ ì¡°ì§ì´ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•´ì•¼ í•  ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒ ìš”ì¸ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ? 
    * ë…ìê°€ ì–»ì–´ì•¼ í•  ìµœì¢…ì ì¸ í†µì°°(Insight)ì„ ì œì‹œí•©ë‹ˆë‹¤.
ì£¼ì˜ì‚¬í•­ : ë‚ ì§œ, ìˆ˜ì‹ ì, ì°¸ì¡° ë“± ë³´ê³ ì„œ í—¤ë” ì •ë³´ëŠ” ìƒì„±í•˜ì§€ ì•Šê³  ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•©ë‹ˆë‹¤.
"""
    user_prompt = f"ì•„ë˜ì˜ ë‰´ìŠ¤ ìš”ì•½ë³¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n---## ìš”ì•½ë³¸ ì‹œì‘ ##---\n\n{full_summary_text}"

    def generate_content_sync():
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            # ë³´ê³ ì„œ ì „ì²´ë¥¼ ìƒì„±í•´ì•¼ í•˜ë¯€ë¡œ ìµœëŒ€ ì¶œë ¥ í† í°ì„ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
            generation_config = {"temperature": 0.2} 
            response = model.generate_content(
                contents=[system_prompt, user_prompt],
                generation_config=generation_config
            )
            if not response.parts:
                raise ValueError(f"Gemini APIê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. (finish_reason: {response.candidates[0].finish_reason.name})")
            return response.text.strip()
        except Exception as e:
            raise Exception(f"ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘ Gemini API ì˜¤ë¥˜: {e}")

    return await asyncio.to_thread(generate_content_sync)


async def run_analysis_and_synthesis_async(filtered_items, progress_callback=None):
    semaphore = asyncio.Semaphore(10)
    successful_results = []
    failed_results = []
    total_items = len(filtered_items)

    async with httpx.AsyncClient() as session:
        tasks = [process_article_task(item, session, semaphore) for item in filtered_items]
        for i, future in enumerate(asyncio.as_completed(tasks)):
            result = await future
            if result and result["status"] == "success":
                successful_results.append(result)
            else:
                failed_results.append(result or {"status": "failed", "reason": "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜", "link": ""})

            if progress_callback:
                progress_callback(i + 1, total_items, f"ğŸ“° ê¸°ì‚¬ ìš”ì•½ ì¤‘... ({i + 1}/{total_items})")

    if not successful_results:
        return None, [], []

    if progress_callback:
        progress_callback(total_items, total_items, "âœ… ë¶„ì„ ì™„ë£Œ! ìµœì¢… ë³´ê³ ì„œë¥¼ ì¢…í•©í•©ë‹ˆë‹¤...")

    final_report = await synthesize_final_report(successful_results)

    return final_report, successful_results, failed_results


# --- Word ì €ì¥ ë¡œì§ ---
def save_summary_to_word(summary_text, successful_results, output_stream):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì„œì‹ì´ ì ìš©ëœ Word ë¬¸ì„œë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    doc = Document()
    style = doc.styles["Normal"]
    style.font.name = "ë§‘ì€ ê³ ë”•"
    style.paragraph_format.line_spacing = 1.5
    style.paragraph_format.space_after = Pt(5)
    
    title_p = doc.add_paragraph()
    title_run = title_p.add_run("AI ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸")
    title_run.bold = True
    title_run.font.size = Pt(20)
    title_p.alignment = 1

    today_str = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
    date_p = doc.add_paragraph()
    date_run = date_p.add_run(f"ì‘ì„±ì¼: {today_str}")
    date_run.font.size = Pt(11)
    date_p.alignment = 2

    doc.add_paragraph("---")

    lines = summary_text.split("\n")
    for line in lines:
        line = line.strip()
        if not line or line == "---":
            continue

        p = None # ë‹¨ë½ ë³€ìˆ˜ ì´ˆê¸°í™”

        if line.startswith("### "):
            p = doc.add_paragraph()
            p.add_run(line.replace("### ", "")).bold = True
            p.runs[0].font.size = Pt(12)
        elif line.startswith("## "):
            p = doc.add_paragraph()
            p.add_run(line.replace("## ", "")).bold = True
            p.runs[0].font.size = Pt(14)
        elif line.startswith("# ") or line.startswith("ğŸ“Œ") or line.startswith("ğŸ“°") or line.startswith("ğŸ“Š") or line.startswith("ğŸ§ "):
            p = doc.add_paragraph()
            p.add_run(line.lstrip("# ğŸ“ŒğŸ“°ğŸ“ŠğŸ§ ").strip()).bold = True
            p.runs[0].font.size = Pt(16)
        elif line.startswith("* "):
            p = doc.add_paragraph(style="List Bullet")
            p.paragraph_format.left_indent = Pt(20) 
            # [ìˆ˜ì •] ë¦¬ìŠ¤íŠ¸ í•­ëª©ì—ì„œ ** ì œê±° ë° êµµì€ ê¸€ì”¨ ì²˜ë¦¬
            clean_line = line.replace("* ", "").replace("**", "")
            p.add_run(clean_line)
        else:
            p = doc.add_paragraph()
            # [ìˆ˜ì •] ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ ** ì œê±° ë° êµµì€ ê¸€ì”¨ ì²˜ë¦¬
            parts = re.split(r'(\*\*.*?\*\*)', line)
            for part in parts:
                if part.startswith('**') and part.endswith('**'):
                    p.add_run(part[2:-2]).bold = True
                else:
                    p.add_run(part)

    doc.add_page_break()
    p = doc.add_paragraph()
    run = p.add_run("ğŸ“ ì°¸ê³  ë‰´ìŠ¤ ëª©ë¡")
    run.bold = True
    run.font.size = Pt(16)

    for idx, result in enumerate(successful_results, 1):
        p = doc.add_paragraph()
        p.add_run(f"{idx}. ")
        add_hyperlink(p, result["link"], result["title"])
        origin = extract_news_source(result["link"])
        pubdate = extract_pubdate_from_item(result["original_item"])
        info = f" ({origin}" + (f", {pubdate}" if pubdate else "") + ")"
        p.add_run(info)

    doc.save(output_stream)


def add_hyperlink(paragraph, url, text):
    part = paragraph.part
    r_id = part.relate_to(url, RT.HYPERLINK, is_external=True)
    hyperlink = OxmlElement("w:hyperlink")
    hyperlink.set(qn("r:id"), r_id)
    new_run = OxmlElement("w:r")
    rPr = OxmlElement("w:rPr")
    color = OxmlElement("w:color")
    color.set(qn("w:val"), "0000FF")
    rPr.append(color)
    underline = OxmlElement("w:u")
    underline.set(qn("w:val"), "single")
    rPr.append(underline)
    new_run.append(rPr)
    text_elem = OxmlElement("w:t")
    text_elem.text = text
    new_run.append(text_elem)
    hyperlink.append(new_run)
    paragraph._p.append(hyperlink)


def extract_news_source(link):
    try:
        netloc = urlparse(link).netloc
        parts = netloc.replace("www.", "").split(".")
        domain = parts[-2] if len(parts) > 1 else parts[0]
        source_map = {
            "chosun": "ì¡°ì„ ì¼ë³´",
            "donga": "ë™ì•„ì¼ë³´",
            "mk": "ë§¤ì¼ê²½ì œ",
            "joongang": "ì¤‘ì•™ì¼ë³´",
            "hani": "í•œê²¨ë ˆ",
            "yna": "ì—°í•©ë‰´ìŠ¤",
            "inews24": "ì•„ì´ë‰´ìŠ¤24",
            "fnnews": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
            "naver": "ë„¤ì´ë²„ë‰´ìŠ¤",
        }
        return source_map.get(domain, domain)
    except:
        return "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"


def extract_pubdate_from_item(item):
    if "pubDate" in item:
        try:
            dt = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d")
        except:
            return None
    return None
