import requests
from bs4 import BeautifulSoup
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv
from docx import Document
from docx.shared import Pt
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.opc.constants import RELATIONSHIP_TYPE as RT
from urllib.parse import urlparse
from datetime import datetime
import re
import asyncio
import httpx
from tqdm.asyncio import tqdm
import json

# --- [NEW] crawl4ai and Pydantic imports ---
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai import CrawlerRunConfig, BrowserConfig
from crawl4ai import LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
# ---------------------------------------------

# Load environment variables
load_dotenv()

# ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Google Gemini ì„¤ì • - í•„ìˆ˜ ì‚¬ìš©
try:
    import google.generativeai as genai
    # Using GOOGLE_API_KEY to align with genai and crawl4ai usage
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key or not google_api_key.strip():
        raise ValueError("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=google_api_key)
    print("âœ… Gemini API ì„¤ì • ì™„ë£Œ")
except ImportError:
    raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-generativeai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
except Exception as e:
    raise Exception(f"Gemini API ì„¤ì • ì‹¤íŒ¨: {e}")

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")


# --- Pydantic model for crawl4ai data structure ---
class NewsArticle(BaseModel):
    title: str = Field(..., description="The main headline or title of the news article.")
    content: str = Field(..., description="The full body text of the news article, excluding ads, comments, and navigation links.")
# ----------------------------------------------------



### ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ (Streamlitì—ì„œ í˜¸ì¶œ)

async def extract_initial_article_content_async(url: str):
    """
    ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ê¸°ì¤€ì´ ë˜ëŠ” ì²« ê¸°ì‚¬ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    crawl4aië¥¼ ì‚¬ìš©í•˜ì—¬ ì–´ë–¤ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë“  ì•ˆì •ì ìœ¼ë¡œ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    config = CrawlerRunConfig(
        extraction_strategy=LLMExtractionStrategy(
            llm_config=LLMConfig(
                provider="gemini/gemini-1.5-flash",
                api_token=os.getenv("GOOGLE_API_KEY")
            ),
            schema=NewsArticle.model_json_schema(),
            instruction="""Extract the title and the main content of the news article.
            Focus only on the article's body, ignoring comments, related articles, and advertisements.
            Return the result in JSON format based on the provided schema.""",
        ),
        cache_mode=CacheMode.DISABLED
    )
    try:
        async with AsyncWebCrawler(verbose=False) as crawler:
            result = await crawler.arun(url=url, config=config)

        if result.success and result.extracted_content:
            extracted_data = json.loads(result.extracted_content)

            # --- [ìˆ˜ì •ë¨] LLMì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°ì— ëŒ€í•œ ë°©ì–´ ì½”ë“œ ì¶”ê°€ ---
            article_dict = None
            if isinstance(extracted_data, list) and extracted_data:
                article_dict = extracted_data[0]
            elif isinstance(extracted_data, dict):
                article_dict = extracted_data

            if article_dict:
                title = article_dict.get("title")
                content = article_dict.get("content")
                if not title or not content:
                    raise Exception("crawl4aiê°€ ì´ˆê¸° ê¸°ì‚¬ì—ì„œ ìœ íš¨í•œ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                print("âœ… crawl4ai ì´ˆê¸° ê¸°ì‚¬ ì¶”ì¶œ ì„±ê³µ!")
                return title, content
            # ----------------------------------------------------------
            
            raise Exception("crawl4aiê°€ ì´ˆê¸° ê¸°ì‚¬ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ crawl4ai ì´ˆê¸° ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {result.error_message}")
            raise Exception(f"crawl4ai ì´ˆê¸° ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {result.error_message}")
    except Exception as e:
        print(f"âŒ ì´ˆê¸° ê¸°ì‚¬ ì¶”ì¶œ ì „ì²´ ê³¼ì • ì‹¤íŒ¨: {e}")
        raise

async def extract_keywords_with_gemini(title, content, max_count=5):
    """Geminië¥¼ ì‚¬ìš©í•´ ë¹„ë™ê¸°ì ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    prompt = f"""
ë‹¤ìŒì€ ë‰´ìŠ¤ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë¥¼ ìµœëŒ€ {max_count}ê°œê¹Œì§€ í•œê¸€ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ê° í‚¤ì›Œë“œëŠ” ëª…ì‚¬ í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.
- êµ¬ë¶„ìëŠ” ì‰¼í‘œ(,) ë˜ëŠ” ì¤„ë°”ê¿ˆì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë³¸ë¬¸: {content}
"""
    
    def generate_keywords_sync():
        try:
            model = genai.GenerativeModel('gemini-1.5-flash')
            contents = [prompt]
            generation_config = {"temperature": 0.2}

            response = model.generate_content(
                contents=contents,
                generation_config=generation_config
            )

            if not response.parts:
                print("âš ï¸ Gemini APIê°€ í‚¤ì›Œë“œì— ëŒ€í•´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                return ""
            return response.text.strip()
        except Exception as e:
            print(f"âŒ Gemini í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    try:
        keywords_text = await asyncio.to_thread(generate_keywords_sync)
        if not keywords_text:
            raise Exception("Geminië¡œë¶€í„° í‚¤ì›Œë“œë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        lines = keywords_text.split("\n")
        keywords_list = []
        for line in lines:
            keywords_list.extend([kw.strip() for kw in line.split(",")])

        cleaned_keywords = [
            re.sub(r"^\s*[\d\.\-]+\s*", "", kw).strip()
            for kw in keywords_list
            if kw.strip()
        ]

        return cleaned_keywords[:max_count]
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def search_news_naver(keywords, display=50):
    """ë„¤ì´ë²„ APIë¥¼ í†µí•´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    query = " ".join(keywords)
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {"query": query, "display": display, "sort": "date"}
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        return response.json()["items"]
    except requests.RequestException as e:
        print(f"âš ï¸ ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        raise


def filter_news_by_date(news_items, start_date, end_date):
    """ê²€ìƒ‰ëœ ë‰´ìŠ¤ë¥¼ ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤."""
    filtered_items = []
    for item in news_items:
        pub_date_str = item.get("pubDate")
        if not pub_date_str:
            continue
        try:
            pub_date = datetime.strptime(
                pub_date_str, "%a, %d %b %Y %H:%M:%S %z"
            ).date()
            if start_date <= pub_date <= end_date:
                filtered_items.append(item)
        except ValueError:
            continue
    return filtered_items

# [ì¶”ê°€] ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© ë¹ ë¥¸ ìŠ¤í¬ë˜í¼ í•¨ìˆ˜
async def extract_naver_article_fast_async(link: str):
    """
    n.news.naver.com ë§í¬ì— ëŒ€í•´ httpxì™€ BeautifulSoupë¥¼ ì‚¬ìš©í•´ ë¹ ë¥´ê²Œ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        async with httpx.AsyncClient() as session:
            response = await session.get(link, headers=headers, timeout=10, follow_redirects=True)
            response.raise_for_status()
            
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ íƒ€ì´í‹€ ì¶”ì¶œ
        title_tag = soup.find("meta", property="og:title")
        title = title_tag["content"].strip() if title_tag else "ì œëª© ì—†ìŒ"
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì„ íƒì (ê°€ì¥ ì¼ë°˜ì ì¸ 2ê°€ì§€)
        content_area = soup.select_one("article#dic_area, div#newsct_article")
        
        if content_area:
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° (ì˜ˆ: ê¸°ì ì •ë³´, ì €ì‘ê¶Œ ë¬¸êµ¬ ë“±)
            for el in content_area.select("span.byline, div.journalist_area, p.copyright"):
                el.decompose()
            content = content_area.get_text(separator="\n", strip=True)
            # ì„±ê³µ ì‹œ ì œëª©ê³¼ ë³¸ë¬¸ ë°˜í™˜
            if content:
                print(f"âœ… ë¹ ë¥¸ ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {link}")
                return title, content

    except Exception as e:
        print(f"âš ï¸ ë¹ ë¥¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {link}, ì˜¤ë¥˜: {e}")
        pass # ì‹¤íŒ¨ ì‹œ ì•„ë˜ crawl4aiê°€ ì²˜ë¦¬í•˜ë„ë¡ None ë°˜í™˜
        
    return None, None

async def extract_article_content_async(link: str):
    """
    crawl4aië¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    config = CrawlerRunConfig(
        extraction_strategy=LLMExtractionStrategy(
            llm_config=LLMConfig(
                provider="gemini/gemini-1.5-flash",
                api_token=google_api_key
            ),
            schema=NewsArticle.model_json_schema(),
            instruction="""Extract the title and the main content of the news article.
            Focus only on the article's body, ignoring comments, related articles, and advertisements.
            Return the result in JSON format based on the provided schema.""",
        ),
    )
    try:
        async with AsyncWebCrawler(verbose=False) as crawler:
            result = await crawler.arun(url=link, config=config)
        
        if result.success and result.extracted_content:
            extracted_data = json.loads(result.extracted_content)

            # --- [ìˆ˜ì •ë¨] LLMì´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš°ì— ëŒ€í•œ ë°©ì–´ ì½”ë“œ ì¶”ê°€ ---
            article_dict = None
            if isinstance(extracted_data, list) and extracted_data:
                article_dict = extracted_data[0]
            elif isinstance(extracted_data, dict):
                article_dict = extracted_data
            
            if article_dict:
                return article_dict.get("title"), article_dict.get("content")
            # ----------------------------------------------------------

        return None, None
    except Exception:
        return None, None


async def summarize_individual_article_async(title, content):
    prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ì•„ë˜ í•­ëª©ì— ë§ì¶”ì–´ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜. ê° í•­ëª©ì€ 2~3 ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
- **ì‚¬ê±´/ì£¼ì œ**: \n- **ì£¼ìš” ì¸ë¬¼/ê¸°ê´€**: \n- **í•µì‹¬ ì£¼ì¥/ë‚´ìš©**: \n- **ê²°ê³¼/ì˜í–¥**:
---
ì œëª©: {title}\në³¸ë¬¸: {content}
"""
    def generate_summary_sync():
        try:
            model = genai.GenerativeModel('gemini-1.5-flash')
            system_prompt = "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ì˜ í•µì‹¬ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤."
            contents = [system_prompt, prompt]
            generation_config = {"temperature": 0.2}

            response = model.generate_content(
                contents=contents,
                generation_config=generation_config
            )

            if not response.parts:
                print("âš ï¸ Gemini APIê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                return None
            return response.text.strip()
        except Exception as e:
            print(f"âŒ Gemini ê°œë³„ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    try:
        summary = await asyncio.to_thread(generate_summary_sync)
        return summary
    except Exception:
        return None


async def process_article_task(item, semaphore):
    async with semaphore:
        # [ìˆ˜ì •] ë„¤ì´ë²„ ë§í¬ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ë„ë¡ ìˆœì„œë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.
        link = item.get("link", item.get("originallink"))
        
        title, content = None, None

        # 1. n.news.naver.com ë§í¬ì¸ ê²½ìš°, ë¨¼ì € ë¹ ë¥¸ ë„¤ì´ë²„ ì „ìš© ìŠ¤í¬ë˜í¼ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.
        if "n.news.naver.com" in link:
            title, content = await extract_naver_article_fast_async(link)

        # 2. ë¹ ë¥¸ ìŠ¤í¬ë˜í•‘ì— ì‹¤íŒ¨í–ˆê±°ë‚˜, ë„¤ì´ë²„ ë§í¬ê°€ ì•„ë‹ ê²½ìš°, crawl4aië¥¼ ì´ìš©í•œ ê°•ë ¥í•œ í´ë°±(Fallback)ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        if not title or not content:
            print(f"â¡ï¸ crawl4ai í´ë°± ì‹¤í–‰: {link}")
            title, content = await extract_article_content_async(link)
        
        # ë‘ ë°©ì‹ ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš°
        if not title or not content:
            return {"status": "failed", "reason": "í¬ë¡¤ë§ ì‹¤íŒ¨", "link": link}
        
        # ìš”ì•½ ì§„í–‰
        summary = await summarize_individual_article_async(title, content)
        if not summary:
            return {"status": "failed", "reason": "ê°œë³„ ìš”ì•½ ì‹¤íŒ¨", "link": link}
            
        return {
            "status": "success",
            "title": re.sub("<.*?>", "", item["title"]),
            "link": link,
            "original_item": item,
            "summary": summary,
        }


async def synthesize_final_report(summaries):
    MAX_INPUT_CHARS = 25000 
    
    full_summary_text = ""
    processed_count = 0
    for summary_data in summaries:
        summary_entry = f"### ë‰´ìŠ¤ {processed_count + 1}: {summary_data['title']}\n{summary_data['summary']}\n\n---\n\n"
        if len(full_summary_text) + len(summary_entry) > MAX_INPUT_CHARS:
            break
        full_summary_text += summary_entry
        processed_count += 1

    if processed_count < len(summaries):
        remaining_count = len(summaries) - processed_count
        full_summary_text += f"\n... ì™¸ {remaining_count}ê°œì˜ ê´€ë ¨ ê¸°ì‚¬ê°€ ìˆìœ¼ë‚˜, ì•ˆì •ì ì¸ ë¶„ì„ì„ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"

    system_prompt = """
ë‹¹ì‹ ì€ ì •ì¹˜/ê²½ì œ/ì‚°ì—… ë¶„ì•¼ì˜ ìµœê³  ìˆ˜ì¤€ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í•µì‹¬ ìš”ì•½ë³¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, íšŒì‚¬ CFOë‚˜ CEOê°€ ì˜ì‚¬ê²°ì •ì„ ìœ„í•´ ì°¸ê³ í•  ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

# ì§€ì‹œì‚¬í•­
ì•„ë˜ì˜ êµ¬ì¡°ì™€ ì„¸ë¶€ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì „ì²´ì ì¸ ë¶„ëŸ‰ì€ 2000ì ë‚´ì™¸ë¡œ, Key Developmentsì™€ Comparative Analysisì˜ ë¹„ì¤‘ì„ ê°ê° 40% ìˆ˜ì¤€ì„ ìœ ì§€í•©ë‹ˆë‹¤.
ë‹¨, ìµœì¢… ê²°ê³¼ë¬¼ì„ ì¶œë ¥í•  ë•Œ ëª©ë¡ì€ ë²ˆí˜¸(1., 2.)ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì˜¤ì§ ë¶ˆë › í¬ì¸íŠ¸('*')ë¡œë§Œ í†µì¼í•´ì•¼ í•©ë‹ˆë‹¤.

---
## ë³´ê³ ì„œ êµ¬ì¡° ë° ì„¸ë¶€ ì§€ì¹¨
1.  **ğŸ“Œ Executive Summary (í•µì‹¬ ìš”ì•½)**
    * ì „ì²´ ìƒí™©ì„ 1~2 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
2.  **ğŸ“° Key Developments (ì£¼ìš” ë™í–¥ ë° ì‚¬ì‹¤ ë¶„ì„)**
    * ì–´ë–¤ ì‚¬ê±´/í–‰ë™ì´ ìˆì—ˆëŠ”ì§€ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
    * ê³µí†µì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ëŠ” ì›ì¸ê³¼ ë°°ê²½ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
    * í•µì‹¬ì ì¸ ì´í•´ê´€ê³„ì(ì¸ë¬¼, ê¸°ì—…, ê¸°ê´€)ëŠ” ëˆ„êµ¬ì´ë©°, ê·¸ë“¤ì˜ ì…ì¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
3.  **ğŸ“Š Comparative Analysis (ë¹„êµ ë¶„ì„ ë° ì´ìŠˆ ì‹¬ì¸µ íƒêµ¬)**
    * ê¸°ì‚¬ë“¤ ê°„ì˜ ê´€ì  ì°¨ì´ë‚˜ ìƒì¶©ë˜ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
    * ìˆ˜ì¹˜, ë°ì´í„°, ì •ì±… ë³€í™” ë“± ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ í‘œ(Table) í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ì‹œê°ì  ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤. (í•„ìš”ì‹œ)
4.  **ğŸ§  Conclusion & Strategic Implications (ê²°ë¡  ë° ì „ëµì  ì‹œì‚¬ì )**
    * Key Developmentsì™€ Comparative Analysisì˜ ë‚´ìš©ì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•©ë‹ˆë‹¤.
    * ì´ëŸ¬í•œ íë¦„ì´ í–¥í›„ ì‹œì¥/ì‚°ì—…/ì •ì±…ì— ë¯¸ì¹  ì˜í–¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
    * ìš°ë¦¬ ì¡°ì§ì´ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•´ì•¼ í•  ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒ ìš”ì¸ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ? 
    * ë…ìê°€ ì–»ì–´ì•¼ í•  ìµœì¢…ì ì¸ í†µì°°(Insight)ì„ ì œì‹œí•©ë‹ˆë‹¤.
ì£¼ì˜ì‚¬í•­ : ë‚ ì§œ, ìˆ˜ì‹ ì, ì°¸ì¡° ë“± ë³´ê³ ì„œ í—¤ë” ì •ë³´ëŠ” ìƒì„±í•˜ì§€ ì•Šê³  ë³¸ë¬¸ ë‚´ìš©ë§Œ ì‘ì„±í•©ë‹ˆë‹¤.
"""
    user_prompt = f"ì•„ë˜ì˜ ë‰´ìŠ¤ ìš”ì•½ë³¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n---## ìš”ì•½ë³¸ ì‹œì‘ ##---\n\n{full_summary_text}"

    def generate_content_sync():
        try:
            model = genai.GenerativeModel('gemini-1.5-flash')
            generation_config = {"temperature": 0.2} 
            response = model.generate_content(
                contents=[system_prompt, user_prompt],
                generation_config=generation_config
            )
            if not response.parts:
                raise ValueError(f"Gemini APIê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. (finish_reason: {response.candidates[0].finish_reason.name})")
            return response.text.strip()
        except Exception as e:
            raise Exception(f"ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘ Gemini API ì˜¤ë¥˜: {e}")

    return await asyncio.to_thread(generate_content_sync)


async def run_analysis_and_synthesis_async(filtered_items, progress_callback=None):
    semaphore = asyncio.Semaphore(10)
    successful_results = []
    failed_results = []
    total_items = len(filtered_items)

    tasks = [process_article_task(item, semaphore) for item in filtered_items]
    for i, future in enumerate(asyncio.as_completed(tasks)):
        result = await future
        if result and result["status"] == "success":
            successful_results.append(result)
        else:
            failed_results.append(result or {"status": "failed", "reason": "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜", "link": ""})

        if progress_callback:
            progress_callback(i + 1, total_items, f"ğŸ“° ê¸°ì‚¬ ìš”ì•½ ì¤‘... ({i + 1}/{total_items})")

    if not successful_results:
        return None, [], []

    if progress_callback:
        progress_callback(total_items, total_items, "âœ… ë¶„ì„ ì™„ë£Œ! ìµœì¢… ë³´ê³ ì„œë¥¼ ì¢…í•©í•©ë‹ˆë‹¤...")

    final_report = await synthesize_final_report(successful_results)

    return final_report, successful_results, failed_results


# --- Word ì €ì¥ ë¡œì§ ---
def save_summary_to_word(summary_text, successful_results, output_stream):
    """ë¶„ì„ ê²°ê³¼ë¥¼ ì„œì‹ì´ ì ìš©ëœ Word ë¬¸ì„œë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    doc = Document()
    style = doc.styles["Normal"]
    style.font.name = "ë§‘ì€ ê³ ë”•"
    style.paragraph_format.line_spacing = 1.5
    style.paragraph_format.space_after = Pt(5)
    
    title_p = doc.add_paragraph()
    title_run = title_p.add_run("AI ë‰´ìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸")
    title_run.bold = True
    title_run.font.size = Pt(20)
    title_p.alignment = 1

    today_str = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
    date_p = doc.add_paragraph()
    date_run = date_p.add_run(f"ì‘ì„±ì¼: {today_str}")
    date_run.font.size = Pt(11)
    date_p.alignment = 2

    doc.add_paragraph("---")

    lines = summary_text.split("\n")
    for line in lines:
        line = line.strip()
        if not line or line == "---":
            continue

        p = None

        if line.startswith("### "):
            p = doc.add_paragraph()
            p.add_run(line.replace("### ", "")).bold = True
            p.runs[0].font.size = Pt(12)
        elif line.startswith("## "):
            p = doc.add_paragraph()
            p.add_run(line.replace("## ", "")).bold = True
            p.runs[0].font.size = Pt(14)
        elif line.startswith("# ") or line.startswith("ğŸ“Œ") or line.startswith("ğŸ“°") or line.startswith("ğŸ“Š") or line.startswith("ğŸ§ "):
            p = doc.add_paragraph()
            p.add_run(line.lstrip("# ğŸ“ŒğŸ“°ğŸ“ŠğŸ§ ").strip()).bold = True
            p.runs[0].font.size = Pt(16)
        elif line.startswith("* "):
            p = doc.add_paragraph(style="List Bullet")
            p.paragraph_format.left_indent = Pt(20) 
            clean_line = line.replace("* ", "").replace("**", "")
            p.add_run(clean_line)
        else:
            p = doc.add_paragraph()
            parts = re.split(r'(\*\*.*?\*\*)', line)
            for part in parts:
                if part.startswith('**') and part.endswith('**'):
                    p.add_run(part[2:-2]).bold = True
                else:
                    p.add_run(part)

    doc.add_page_break()
    p = doc.add_paragraph()
    run = p.add_run("ğŸ“ ì°¸ê³  ë‰´ìŠ¤ ëª©ë¡")
    run.bold = True
    run.font.size = Pt(16)

    for idx, result in enumerate(successful_results, 1):
        p = doc.add_paragraph()
        p.add_run(f"{idx}. ")
        add_hyperlink(p, result["link"], result["title"])
        origin = extract_news_source(result["link"])
        pubdate = extract_pubdate_from_item(result["original_item"])
        info = f" ({origin}" + (f", {pubdate}" if pubdate else "") + ")"
        p.add_run(info)

    doc.save(output_stream)


def add_hyperlink(paragraph, url, text):
    part = paragraph.part
    r_id = part.relate_to(url, RT.HYPERLINK, is_external=True)
    hyperlink = OxmlElement("w:hyperlink")
    hyperlink.set(qn("r:id"), r_id)
    new_run = OxmlElement("w:r")
    rPr = OxmlElement("w:rPr")
    color = OxmlElement("w:color")
    color.set(qn("w:val"), "0000FF")
    rPr.append(color)
    underline = OxmlElement("w:u")
    underline.set(qn("w:val"), "single")
    rPr.append(underline)
    new_run.append(rPr)
    text_elem = OxmlElement("w:t")
    text_elem.text = text
    new_run.append(text_elem)
    hyperlink.append(new_run)
    paragraph._p.append(hyperlink)


def extract_news_source(link):
    try:
        netloc = urlparse(link).netloc
        parts = netloc.replace("www.", "").split(".")
        domain = parts[-2] if len(parts) > 1 else parts[0]
        source_map = {
            "chosun": "ì¡°ì„ ì¼ë³´",
            "donga": "ë™ì•„ì¼ë³´",
            "mk": "ë§¤ì¼ê²½ì œ",
            "joongang": "ì¤‘ì•™ì¼ë³´",
            "hani": "í•œê²¨ë ˆ",
            "yna": "ì—°í•©ë‰´ìŠ¤",
            "inews24": "ì•„ì´ë‰´ìŠ¤24",
            "fnnews": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
            "naver": "ë„¤ì´ë²„ë‰´ìŠ¤",
        }
        return source_map.get(domain, domain)
    except:
        return "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"


def extract_pubdate_from_item(item):
    if "pubDate" in item:
        try:
            dt = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d")
        except:
            return None
    return None
