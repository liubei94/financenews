import requests
from bs4 import BeautifulSoup
from openai import AsyncOpenAI
import os
from dotenv import load_dotenv
from docx import Document
from docx.shared import Pt
from docx.oxml import OxmlElement
from docx.oxml.ns import qn
from docx.opc.constants import RELATIONSHIP_TYPE as RT
from urllib.parse import urlparse
from datetime import datetime
import re
import asyncio
import httpx
from tqdm.asyncio import tqdm

# Load environment variables
load_dotenv()

# ë¹„ë™ê¸° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Google Gemini ì„¤ì • - í•„ìˆ˜ ì‚¬ìš©
try:
    import google.generativeai as genai
    google_api_key = os.getenv("GOOGLE_API_KEY")
    if not google_api_key or not google_api_key.strip():
        raise ValueError("GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    genai.configure(api_key=google_api_key)
    print("âœ… Gemini API ì„¤ì • ì™„ë£Œ")
except ImportError:
    raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install google-generativeai'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
except Exception as e:
    raise Exception(f"Gemini API ì„¤ì • ì‹¤íŒ¨: {e}")

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")


### ê¸°ëŠ¥ í•¨ìˆ˜ë“¤ (Streamlitì—ì„œ í˜¸ì¶œ)


def extract_initial_article_content(url):
    """ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ê¸°ì¤€ì´ ë˜ëŠ” ì²« ê¸°ì‚¬ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        title_tag = soup.find("h2", class_="media_end_head_headline")
        title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
        content_tag = soup.select_one("article#dic_area, div#newsct_article")
        paragraphs = content_tag.find_all("p") if content_tag else []
        content = " ".join([p.get_text(strip=True) for p in paragraphs])
        return title, content
    except requests.RequestException as e:
        print(f"âŒ ì´ˆê¸° ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        raise


async def extract_keywords_with_gpt(title, content, max_count=5):
    """GPTë¥¼ ì‚¬ìš©í•´ ë¹„ë™ê¸°ì ìœ¼ë¡œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (íŒŒì‹± ë¡œì§ ê°•í™”)"""
    prompt = f"""
ë‹¤ìŒì€ ë‰´ìŠ¤ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ì˜ í•µì‹¬ ì£¼ì œë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë¥¼ ìµœëŒ€ 5ê°œê¹Œì§€ í•œê¸€ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ê° í‚¤ì›Œë“œëŠ” ëª…ì‚¬ í˜•íƒœë¡œ ê°„ê²°í•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.
- êµ¬ë¶„ìëŠ” ì‰¼í‘œ(,) ë˜ëŠ” ì¤„ë°”ê¿ˆì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

ì œëª©: {title}
ë³¸ë¬¸: {content}
"""
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ìµœëŒ€ 5ê°œê¹Œì§€ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        keywords_text = response.choices[0].message.content.strip()

        lines = keywords_text.split("\n")
        keywords_list = []
        for line in lines:
            keywords_list.extend([kw.strip() for kw in line.split(",")])

        cleaned_keywords = [
            re.sub(r"^\s*[\d\.\-]+\s*", "", kw).strip()
            for kw in keywords_list
            if kw.strip()
        ]

        return cleaned_keywords[:max_count]
    except Exception as e:
        print(f"âŒ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


def search_news_naver(keywords, display=50):
    """ë„¤ì´ë²„ APIë¥¼ í†µí•´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    query = " ".join(keywords)
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    params = {"query": query, "display": display, "sort": "date"}
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        return response.json()["items"]
    except requests.RequestException as e:
        print(f"âš ï¸ ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        raise


def filter_news_by_date(news_items, start_date, end_date):
    """ê²€ìƒ‰ëœ ë‰´ìŠ¤ë¥¼ ì§€ì •ëœ ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§í•©ë‹ˆë‹¤."""
    filtered_items = []
    for item in news_items:
        pub_date_str = item.get("pubDate")
        if not pub_date_str:
            continue
        try:
            pub_date = datetime.strptime(
                pub_date_str, "%a, %d %b %Y %H:%M:%S %z"
            ).date()
            if start_date <= pub_date <= end_date:
                filtered_items.append(item)
        except ValueError:
            continue
    return filtered_items


# --- ë¹„ë™ê¸° ì²˜ë¦¬ í•µì‹¬ ë¡œì§ ---


async def extract_article_content_async(link, session):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    try:
        response = await session.get(
            link, headers=headers, timeout=15, follow_redirects=True
        )
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        content_area = soup.select_one("article#dic_area, div#newsct_article")
        if content_area:
            title_tag = soup.find("meta", property="og:title")
            title = title_tag["content"].strip() if title_tag else "ì œëª© ì—†ìŒ"
            content = " ".join(
                p.get_text(strip=True)
                for p in content_area.find_all("p")
                if p.get_text(strip=True)
            )
            return title, content
        title_tag = soup.find("meta", property="og:title")
        title = title_tag["content"].strip() if title_tag else soup.title.string.strip()
        selectors = [
            "div.article_body",
            "div.article_view",
            "div#article-body",
            "div#news_body_area",
            "div.article_txt",
            "div#article_body",
            "div.article-text",
            "section.article-body",
        ]
        content_area = soup.select_one(", ".join(selectors))
        paragraphs = (
            content_area.find_all("p")
            if content_area
            else soup.find("body").find_all("p")
        )
        content = " ".join(
            [
                p.get_text(strip=True)
                for p in paragraphs
                if len(p.get_text(strip=True)) > 50
            ]
        )
        return title, content
    except Exception:
        return None, None


async def summarize_individual_article_async(title, content):
    prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ ì•„ë˜ í•­ëª©ì— ë§ì¶”ì–´ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì¤˜. ê° í•­ëª©ì€ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì¤˜.
- **ì‚¬ê±´/ì£¼ì œ**: \n- **ì£¼ìš” ì¸ë¬¼/ê¸°ê´€**: \n- **í•µì‹¬ ì£¼ì¥/ë‚´ìš©**: \n- **ê²°ê³¼/ì˜í–¥**:
---
ì œëª©: {title}\në³¸ë¬¸: {content}
"""
    try:
        response = await client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ê¸°ì‚¬ì˜ í•µì‹¬ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì—¬ ìš”ì•½í•©ë‹ˆë‹¤.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        return response.choices[0].message.content.strip()
    except Exception:
        return None


async def process_article_task(item, session, semaphore):
    async with semaphore:
        link = item.get("originallink", item.get("link"))
        title, content = await extract_article_content_async(link, session)
        if not title or not content:
            return {"status": "failed", "reason": "í¬ë¡¤ë§ ì‹¤íŒ¨", "link": link}
        summary = await summarize_individual_article_async(title, content)
        if not summary:
            return {"status": "failed", "reason": "ê°œë³„ ìš”ì•½ ì‹¤íŒ¨", "link": link}
        return {
            "status": "success",
            "title": re.sub("<.*?>", "", item["title"]),
            "link": link,
            "original_item": item,
            "summary": summary,
        }


async def synthesize_final_report(summaries):
    """ëª¨ë“  ë‰´ìŠ¤ ìš”ì•½ë³¸ì„ ë°›ì•„ í•˜ë‚˜ì˜ ì¢…í•© ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # AI ì…ë ¥ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ€ ê¸€ì ìˆ˜ ì œí•œ (í† í° ì•½ 2ë§Œê°œ ë¶„ëŸ‰)
    MAX_INPUT_CHARS = 25000 
    
    full_summary_text = ""
    processed_count = 0
    for summary_data in summaries:
        summary_entry = f"### ë‰´ìŠ¤ {processed_count + 1}: {summary_data['title']}\n{summary_data['summary']}\n\n---\n\n"
        if len(full_summary_text) + len(summary_entry) > MAX_INPUT_CHARS:
            break
        full_summary_text += summary_entry
        processed_count += 1

    if processed_count < len(summaries):
        remaining_count = len(summaries) - processed_count
        full_summary_text += f"\n... ì™¸ {remaining_count}ê°œì˜ ê´€ë ¨ ê¸°ì‚¬ê°€ ìˆìœ¼ë‚˜, ì•ˆì •ì ì¸ ë¶„ì„ì„ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"

    system_prompt = """
ë‹¹ì‹ ì€ ì •ì¹˜/ê²½ì œ/ì‚°ì—… ë¶„ì•¼ì˜ ìµœê³  ìˆ˜ì¤€ì˜ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì—¬ëŸ¬ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í•µì‹¬ ìš”ì•½ë³¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, íšŒì‚¬ CFOë‚˜ CEOê°€ ì˜ì‚¬ê²°ì •ì„ ìœ„í•´ ì°¸ê³ í•  ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ì§€ì¼œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì „ì²´ì ì¸ ë¶„ëŸ‰ì€ 2000ì ë‚´ì™¸ë¡œ, Key Developmentsì™€ Comparative Analysisì˜ ë¹„ì¤‘ì„ 80% ì´ìƒ ìœ ì§€í•©ë‹ˆë‹¤.
1.  **ğŸ“Œ Executive Summary (í•µì‹¬ ìš”ì•½)**
    * ì „ì²´ ìƒí™©ì„ 1~2 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
2.  **ğŸ“° Key Developments (ì£¼ìš” ë™í–¥ ë° ì‚¬ì‹¤ ë¶„ì„)**
    * ì–´ë–¤ ì‚¬ê±´/í–‰ë™ì´ ìˆì—ˆëŠ”ì§€ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
    * ê³µí†µì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ëŠ” ì›ì¸ê³¼ ë°°ê²½ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
    * í•µì‹¬ì ì¸ ì´í•´ê´€ê³„ì(ì¸ë¬¼, ê¸°ì—…, ê¸°ê´€)ëŠ” ëˆ„êµ¬ì´ë©°, ê·¸ë“¤ì˜ ì…ì¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
3.  **ğŸ“Š Comparative Analysis (ë¹„êµ ë¶„ì„ ë° ì´ìŠˆ ì‹¬ì¸µ íƒêµ¬)**
    * ê¸°ì‚¬ë“¤ ê°„ì˜ ê´€ì  ì°¨ì´ë‚˜ ìƒì¶©ë˜ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
    * ìˆ˜ì¹˜, ë°ì´í„°, ì •ì±… ë³€í™” ë“± ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ í‘œ(Table) í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ì‹œê°ì  ì´í•´ë¥¼ ë•ìŠµë‹ˆë‹¤. (í•„ìš”ì‹œ)
4.  **ğŸ§  Conclusion & Strategic Implications (ê²°ë¡  ë° ì „ëµì  ì‹œì‚¬ì )**
    * Key Developmentsì™€ Comparative Analysisì˜ ë‚´ìš©ì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•©ë‹ˆë‹¤.
    * ì´ëŸ¬í•œ íë¦„ì´ í–¥í›„ ì‹œì¥/ì‚°ì—…/ì •ì±…ì— ë¯¸ì¹  ì˜í–¥ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
    * ìš°ë¦¬ ì¡°ì§ì´ ì£¼ì˜ ê¹Šê²Œ ê´€ì°°í•´ì•¼ í•  ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒ ìš”ì¸ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ? 
    * ë…ìê°€ ì–»ì–´ì•¼ í•  ìµœì¢…ì ì¸ í†µì°°(Insight)ì„ ì œì‹œí•©ë‹ˆë‹¤.
"""
    user_prompt = f"ì•„ë˜ì˜ ë‰´ìŠ¤ ìš”ì•½ë³¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n---## ìš”ì•½ë³¸ ì‹œì‘ ##---\n\n{full_summary_text}"

    def generate_content_sync():
        try:
            model = genai.GenerativeModel('gemini-2.5-flash')
            # ë³´ê³ ì„œ ì „ì²´ë¥¼ ìƒì„±í•´ì•¼ í•˜ë¯€ë¡œ ìµœëŒ€ ì¶œë ¥ í† í°ì„ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
            generation_config = {"temperature": 0.2} 
            response = model.generate_content(
                contents=[system_prompt, user_prompt],
                generation_config=generation_config
            )
            if not response.parts:
                raise ValueError(f"Gemini APIê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. (finish_reason: {response.candidates[0].finish_reason.name})")
            return response.text.strip()
        except Exception as e:
            raise Exception(f"ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘ Gemini API ì˜¤ë¥˜: {e}")

    return await asyncio.to_thread(generate_content_sync)


async def run_analysis_and_synthesis_async(filtered_items, progress_callback=None):
    semaphore = asyncio.Semaphore(10)
    successful_results = []
    failed_results = []
    total_items = len(filtered_items)

    async with httpx.AsyncClient() as session:
        tasks = [process_article_task(item, session, semaphore) for item in filtered_items]
        for i, future in enumerate(asyncio.as_completed(tasks)):
            result = await future
            if result and result["status"] == "success":
                successful_results.append(result)
            else:
                failed_results.append(result or {"status": "failed", "reason": "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜", "link": ""})

            if progress_callback:
                progress_callback(i + 1, total_items, f"ğŸ“° ê¸°ì‚¬ ìš”ì•½ ì¤‘... ({i + 1}/{total_items})")

    if not successful_results:
        return None, [], []

    if progress_callback:
        progress_callback(total_items, total_items, "âœ… ë¶„ì„ ì™„ë£Œ! ìµœì¢… ë³´ê³ ì„œë¥¼ ì¢…í•©í•©ë‹ˆë‹¤...")

    final_report = await synthesize_final_report(successful_results)

    return final_report, successful_results, failed_results


# --- Word ì €ì¥ ë¡œì§ ---
def save_summary_to_word(summary_text, successful_results, output_stream):
    doc = Document()
    style = doc.styles["Normal"]
    font = style.font
    font.name = "ë§‘ì€ ê³ ë”•"
    style.paragraph_format.line_spacing = 1.5
    style.paragraph_format.space_after = Pt(5)

    lines = summary_text.split("\n")
    for line in lines:
        line = line.strip()
        if not line:
            continue
        if line.startswith("### "):
            p = doc.add_paragraph()
            run = p.add_run(line.replace("### ", ""))
            run.bold = True
            run.font.size = Pt(12)
        elif line.startswith("## "):
            p = doc.add_paragraph()
            run = p.add_run(line.replace("## ", ""))
            run.bold = True
            run.font.size = Pt(14)
        elif line.startswith("# "):
            p = doc.add_paragraph()
            run = p.add_run(line.replace("# ", ""))
            run.bold = True
            run.font.size = Pt(16)
        elif line.startswith("* "):
            p = doc.add_paragraph(style="List Bullet")
            parts = re.split(r"(\*\*.*?\*\*)", line.replace("* ", ""))
            for part in parts:
                if part.startswith("**") and part.endswith("**"):
                    run = p.add_run(part[2:-2])
                    run.bold = True
                else:
                    p.add_run(part)
        else:
            p = doc.add_paragraph()
            parts = re.split(r"(\*\*.*?\*\*)", line)
            for part in parts:
                if part.startswith("**") and part.endswith("**"):
                    run = p.add_run(part[2:-2])
                    run.bold = True
                else:
                    p.add_run(part)

    doc.add_page_break()
    p = doc.add_paragraph()
    run = p.add_run("ğŸ“ ì°¸ê³  ë‰´ìŠ¤ ëª©ë¡")
    run.bold = True
    run.font.size = Pt(16)

    for idx, result in enumerate(successful_results, 1):
        p = doc.add_paragraph()
        p.add_run(f"{idx}. ")
        add_hyperlink(p, result["link"], result["title"])
        origin = extract_news_source(result["link"])
        pubdate = extract_pubdate_from_item(result["original_item"])
        info = f" ({origin}" + (f", {pubdate}" if pubdate else "") + ")"
        p.add_run(info)

    doc.save(output_stream)


def add_hyperlink(paragraph, url, text):
    part = paragraph.part
    r_id = part.relate_to(url, RT.HYPERLINK, is_external=True)
    hyperlink = OxmlElement("w:hyperlink")
    hyperlink.set(qn("r:id"), r_id)
    new_run = OxmlElement("w:r")
    rPr = OxmlElement("w:rPr")
    color = OxmlElement("w:color")
    color.set(qn("w:val"), "0000FF")
    rPr.append(color)
    underline = OxmlElement("w:u")
    underline.set(qn("w:val"), "single")
    rPr.append(underline)
    new_run.append(rPr)
    text_elem = OxmlElement("w:t")
    text_elem.text = text
    new_run.append(text_elem)
    hyperlink.append(new_run)
    paragraph._p.append(hyperlink)


def extract_news_source(link):
    try:
        netloc = urlparse(link).netloc
        parts = netloc.replace("www.", "").split(".")
        domain = parts[-2] if len(parts) > 1 else parts[0]
        source_map = {
            "chosun": "ì¡°ì„ ì¼ë³´",
            "donga": "ë™ì•„ì¼ë³´",
            "mk": "ë§¤ì¼ê²½ì œ",
            "joongang": "ì¤‘ì•™ì¼ë³´",
            "hani": "í•œê²¨ë ˆ",
            "yna": "ì—°í•©ë‰´ìŠ¤",
            "inews24": "ì•„ì´ë‰´ìŠ¤24",
            "fnnews": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
            "naver": "ë„¤ì´ë²„ë‰´ìŠ¤",
        }
        return source_map.get(domain, domain)
    except:
        return "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"


def extract_pubdate_from_item(item):
    if "pubDate" in item:
        try:
            dt = datetime.strptime(item["pubDate"], "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d")
        except:
            return None
    return None
